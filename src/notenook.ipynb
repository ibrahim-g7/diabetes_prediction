{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os, warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import marimo as mo\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "plt.style.use('default')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "1. Dataset: Comprehensive dataset of medical data associated with 13 distinct type of disbetes.\n",
    "2. Objective: Apply different classification models to the dataset, while applying cross validation methods and feature selection technique if neede to improve the performance of the models.\n",
    "3. Model Evluation.\n",
    "4. Report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used functions\n",
    "# train, validaition, and test split\n",
    "\n",
    "def get_scores(y_actual, y_pred):\n",
    "    f1 = f1_score(y_actual, y_pred, average=\"macro\")\n",
    "    acc = accuracy_score(y_actual, y_pred)\n",
    "    return f1, acc\n",
    "\n",
    "\n",
    "# Preprocessing \n",
    "def preprocessing(train, validation):\n",
    "    X_train = train.copy()\n",
    "    X_val = validation.copy()\n",
    "    cate_features = X_train.select_dtypes(include=\"object\").columns.to_list()\n",
    "    num_features = X_train.select_dtypes(include=\"int64\").columns.to_list()\n",
    "    # Initilize scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train[num_features] = scaler.fit_transform(X_train[num_features])\n",
    "    X_val[num_features] = scaler.transform(X_val[num_features])\n",
    "\n",
    "    # Initialize encoder \n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    # Transform training set\n",
    "    encoded_train_data = encoder.fit_transform(X_train[cate_features])\n",
    "    # Make encoded data frame for training set\n",
    "    encoded_train = pd.DataFrame(\n",
    "        encoded_train_data,\n",
    "        columns=encoder.get_feature_names_out(cate_features),\n",
    "        index=X_train.index\n",
    "    )\n",
    "    # Apply the training encoding and make encoded dataframe of the validation set\n",
    "    encoded_val_data = encoder.transform(X_val[cate_features])\n",
    "    encoded_val = pd.DataFrame(\n",
    "        encoded_val_data,\n",
    "        columns=encoder.get_feature_names_out(cate_features),\n",
    "        index=X_val.index\n",
    "    )\n",
    "\n",
    "    # Combine the numerical and encoded columns for both the training and validation set. \n",
    "    X_train_processed = pd.concat([X_train[num_features], encoded_train], axis=1)\n",
    "    X_val_processed = pd.concat([X_val[num_features], encoded_val], axis=1)\n",
    "\n",
    "    return X_train_processed, X_val_processed\n",
    "\n",
    "def cross_validation(X_train, y_train):\n",
    "    models = {\n",
    "        \"Logistic Regresion\": LogisticRegression(penalty=\"l2\", \n",
    "                                                 solver=\"saga\",\n",
    "                                                 max_iter=5000,\n",
    "                                                 random_state=76, n_jobs=-1),\n",
    "        \"Linear SVC\": LinearSVC(dual=False, \n",
    "                         max_iter=2000,\n",
    "                         random_state=76),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(max_depth=5,\n",
    "                                                min_samples_leaf=5,\n",
    "                                                ccp_alpha=0.01,\n",
    "                                                random_state=76),\n",
    "        \"Random Forest\": RandomForestClassifier(max_depth=7, \n",
    "                                                min_samples_leaf=5,\n",
    "                                                random_state=76, n_jobs=-1),\n",
    "        \"AdaBoost\": AdaBoostClassifier(random_state=76)\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        results = cross_validate(\n",
    "            model, \n",
    "            X_train, y_train,\n",
    "            cv=5, \n",
    "            scoring = [\"f1_macro\", \"accuracy\"],\n",
    "            n_jobs=-1,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        print(f\"{name}:\")\n",
    "        print(\"  F1 (val):\", results['test_f1_macro'])\n",
    "        print(\"  F1 (train):\", results['train_f1_macro'])\n",
    "        print(\"  Acc (val):\", results['test_accuracy'])\n",
    "        print(\"  Acc (train):\", results['train_accuracy'])\n",
    "        print(f\"------------------------------------\")\n",
    "        f1_scores = {\n",
    "            \"Training\": results[\"train_f1_macro\"],\n",
    "            \"Test\": results[\"test_f1_macro\"]\n",
    "        }\n",
    "        acc_scores = {\n",
    "            \"Training\": results[\"train_accuracy\"],\n",
    "            \"Test\": results[\"test_accuracy\"]\n",
    "        }\n",
    "\n",
    "        x = np.arange(5)\n",
    "        width = 0.3\n",
    "\n",
    "        fig, axes =  plt.subplots(1, 2, figsize=(18,6))\n",
    "        fig.suptitle(f\"5-Fold scores using {name} model.\")\n",
    "        axes[0].set_title(\"F1 Scores\")\n",
    "        axes[0].set_xlabel(\"Folds\")\n",
    "        axes[0].set_ylabel(\"F1 Score\")\n",
    "        axes[0].bar(x-0.2, f1_scores[\"Training\"], width, label=\"Training\")\n",
    "        axes[0].bar(x+0.2, f1_scores[\"Test\"], width, label=\"Validation\")\n",
    "        axes[0].set_ylim(0,1)\n",
    "        axes[0].legend()\n",
    "\n",
    "        axes[1].set_title(\"Accuracy Score\")\n",
    "        axes[1].set_xlabel(\"Folds\")\n",
    "        axes[1].set_ylabel(\"Accuracy\")\n",
    "        axes[1].bar(x-0.2, acc_scores[\"Training\"], width, label=\"Training\")\n",
    "        axes[1].bar(x+0.2, acc_scores[\"Test\"], width, label=\"Validation\")\n",
    "        axes[1].set_ylim(0,1)\n",
    "        axes[1].legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories \n",
    "main_dir = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(main_dir, \"data\")\n",
    "data_path = os.path.join(data_dir, \"diabetes_dataset00.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {},
   "source": [
    "# Datasets:\n",
    "\n",
    "\n",
    "1. The dataset contains 34 features, and 70000 observation with no null values, it includes 13 numerical features and 21 categorical features.\n",
    "2. We note the the distribution of classes in the target feature is mostly evenly distributed, ~ 5000 observation for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "_count = df[\"Target\"].value_counts()\n",
    "plt.figure(figsize=(12, 6))  # width=10, height=6 inches\n",
    "sns.barplot(_count)\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate labels 45 degrees, align right\n",
    "plt.tight_layout()                   # Adjust layout to prevent label cutoff\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define numerical, and categorical features\n",
    "_cate_features = df.select_dtypes(include=\"object\").columns.to_list()\n",
    "_num_features = df.select_dtypes(include=\"int64\").columns.to_list()\n",
    "_features = []\n",
    "_count = []\n",
    "for feature in _cate_features: \n",
    "    print(f\"The feature {feature} has {df[feature].nunique()} unique entries.\")\n",
    "    _features.append(feature)\n",
    "    _count.append(df[feature].nunique())\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x= _features,\n",
    "           y= _count)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {},
   "source": [
    "# Data preperation.\n",
    "\n",
    "1. Given that all categorical columns are low in cardinality, i will use one hot encoder to them all, and standard scaler for numerical features.\n",
    "2. I will do 68%, 17%, 15% training, validation and test split. Spliting the training and validation set into 5 equal folds after keeping 15% as test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns=\"Target\", axis=1).copy(), df[\"Target\"].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=76)\n",
    "\n",
    "X_train_processed, X_test_processed = preprocessing(X_train, X_test)\n",
    "cross_validation(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "As shown from the above graphs, the scores seem equal across all folds. But there is a discrepenacy in performance across different models. Chosing random forest being the highest performant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {},
   "source": [
    "# Improved Model\n",
    "\n",
    "To improve the performance of random forest, I will do the following:\n",
    "\n",
    "1. Feature selection.\n",
    "2. Hyperparameter tuning (Grid Search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest model \n",
    "rf = RandomForestClassifier(max_depth=7, \n",
    "                            min_samples_leaf=5,\n",
    "                            random_state=76, n_jobs=-1)\n",
    "rf.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort most important features, and plot top 20 \n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train_processed.columns\n",
    "feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "sns.barplot(feat_imp[:20])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train, acc_train = get_scores(y_actual=y_train, y_pred = rf.predict(X_train_processed))\n",
    "f1_test, acc_test = get_scores(y_actual=y_test, y_pred = rf.predict(X_test_processed))\n",
    "print(f\"Training: F1: {f1_train:.3f}, Accuracy: {acc_train:.3f}.\")\n",
    "print(f\"Test: F1: {f1_test:.3f}, Accuracy: {acc_test:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We notice slight improvement from feature selection\n",
    "top_features = feat_imp[:20].index.to_list()\n",
    "rf_feature = RandomForestClassifier(max_depth=7, \n",
    "                            min_samples_leaf=5,\n",
    "                            random_state=76, n_jobs=-1)\n",
    "rf_feature.fit(X_train_processed[top_features], y_train)\n",
    "f1_train_feat, acc_train_feat = get_scores(y_actual=y_train, y_pred = rf_feature.predict(X_train_processed[top_features]))\n",
    "f1_test_feat, acc_test_feat = get_scores(y_actual=y_test, y_pred = rf_feature.predict(X_test_processed[top_features]))\n",
    "print(f\"Training: F1: {f1_train_feat:.3f}, Accuracy: {acc_train_feat:.3f}.\")\n",
    "print(f\"Test: F1: {f1_test_feat:.3f}, Accuracy: {acc_test_feat:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "# Best parameters: {'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 400}\n",
    "param_grid = {\n",
    "    'n_estimators': [400, 600],         # Number of trees\n",
    "    'max_depth': [10, 15],         # Tree depth\n",
    "    'min_samples_split': [4,6],    # Min samples to split a node\n",
    "    'min_samples_leaf': [1],      # Min samples at a leaf node\n",
    "    'max_features': ['sqrt'],   # Number of features to consider at each split\n",
    "    'bootstrap': [False]         # Whether bootstrap samples are used\n",
    "}\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"f1_macro\": \"f1_macro\"\n",
    "}\n",
    "rf_grid = RandomForestClassifier(random_state=76, n_jobs=-1)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf_grid,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=scoring,\n",
    "    refit=\"f1_macro\", \n",
    "    verbose=2, \n",
    "    return_train_score=True\n",
    ")\n",
    "grid.fit(X_train_processed[top_features], y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best f1_macro score:\", grid.best_score_)\n",
    "\n",
    "results = grid.cv_results_\n",
    "print(\"All metrics for best model:\")\n",
    "for metric in scoring:\n",
    "    print(f\"{metric}: {results[f'mean_test_{metric}'][grid.best_index_]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {},
   "source": [
    "Output of the above grid search\n",
    "```\n",
    "Best parameters: {'bootstrap': False, 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 600}\n",
    "Best f1_macro score: 0.9037603761618254\n",
    "All metrics for best model:\n",
    "accuracy: 0.905\n",
    "f1_macro: 0.904\n",
    "```\n",
    "\n",
    "# Final Model\n",
    "\n",
    "1. After thorough analysis, I chose random forest with the above parameters and selected features.\n",
    "2. Finally, after cross validatiion, feature selection, and hyperparameter tunning. I will train the final model with 80% ~ 20% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid.best_params_\n",
    "X_final, y_final = df.drop(\"Target\",axis=1).copy(), df[\"Target\"].copy()\n",
    "X_train_selected, X_test_selected, y_train_selected, y_test_selected = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, random_state=76\n",
    ")\n",
    "X_train_sel_processed, X_test_sel_processed = preprocessing(X_train_selected,\n",
    "                                                            X_test_selected)\n",
    "X_train_sel_processed, X_test_sel_processed = X_train_sel_processed[top_features] , X_test_sel_processed[top_features]\n",
    "\n",
    "rf_best = RandomForestClassifier(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best.fit(X_train_sel_processed, y_train_selected)\n",
    "y_pred = rf_best.predict(X_test_sel_processed)\n",
    "y_pred_train = rf_best.predict(X_train_sel_processed)\n",
    "f1_train_final, acc_train_final = get_scores(y_actual=y_train_selected, \n",
    "                                             y_pred=y_pred_train)\n",
    "f1_test_final, acc_test_final = get_scores(y_actual=y_test_selected,\n",
    "                                           y_pred=y_pred)\n",
    "\n",
    "print(f\"Final Model, training score ~ F1: {f1_train_final:.4f} Accuracy: {acc_train_final:.4f}\")\n",
    "print(f\"Final Model, testing score ~ F1: {f1_test_final:.4f} Accuracy: {acc_test_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
